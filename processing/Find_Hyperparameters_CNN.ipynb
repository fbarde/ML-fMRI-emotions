{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General aim of this Notebook :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general aim of this notebook is to find the best hyperparameters for the Convolutional Neural Network, using GridSearchCV. \n",
    "\n",
    "In order to use it, one needs to load the training datas. \n",
    "\n",
    "The best hyperparameters found are indicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_path = 'C:/Users/Flore.MININT-64PJ14C/Documents/GitHub/ML_Project2/results/Split_400_0.8/Combined_All_Train_80.csv'\n",
    "tx_train = pd.read_csv(data_train_path,sep=\",\",squeeze=True)\n",
    "X_train=tx_train.iloc[:, :400]\n",
    "ytr = pd.read_csv(data_train_path,sep=\",\",usecols=[400],squeeze=True)\n",
    "y_train = tf.keras.utils.to_categorical(ytr,num_classes=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10908, 400)\n",
      "(10908, 14)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, Dropout, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = X_train.shape[0], X_train.shape[1]\n",
    "n_outputs=14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10908, 400)\n",
      "(10908, 400, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "X_train=np.expand_dims(X_train, axis=-1)\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the general model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Conv1D(filters=12, kernel_size=5, activation='relu',input_shape=(n_features, 1)))\n",
    "model.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8, input_dim=199, activation='relu'))\n",
    "model.add(Dense(14, activation='softmax'))\n",
    "#Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best batch size and number of epoch, BEST: bs=1 and epoch=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # create model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Conv1D(filters=12, kernel_size=5, activation='relu',input_shape=(n_features, 1)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(8, input_dim=199, activation='relu'))\n",
    "    model.add(Dense(14, activation='softmax'))\n",
    "    #Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.154132 using {'batch_size': 1, 'epochs': 10}\n",
      "0.154132 (0.073810) with: {'batch_size': 1, 'epochs': 10}\n",
      "0.079439 (0.039836) with: {'batch_size': 1, 'epochs': 50}\n",
      "0.106365 (0.039468) with: {'batch_size': 1, 'epochs': 100}\n",
      "0.108532 (0.046159) with: {'batch_size': 10, 'epochs': 10}\n",
      "0.103889 (0.041429) with: {'batch_size': 10, 'epochs': 50}\n",
      "0.090994 (0.030404) with: {'batch_size': 10, 'epochs': 100}\n",
      "0.086557 (0.039219) with: {'batch_size': 20, 'epochs': 10}\n",
      "0.097493 (0.012256) with: {'batch_size': 20, 'epochs': 50}\n",
      "0.098731 (0.046335) with: {'batch_size': 20, 'epochs': 100}\n",
      "0.102342 (0.046140) with: {'batch_size': 40, 'epochs': 10}\n",
      "0.057670 (0.049711) with: {'batch_size': 40, 'epochs': 50}\n",
      "0.146497 (0.088628) with: {'batch_size': 40, 'epochs': 100}\n",
      "0.083565 (0.041869) with: {'batch_size': 60, 'epochs': 10}\n",
      "0.070670 (0.055715) with: {'batch_size': 60, 'epochs': 50}\n",
      "0.095017 (0.024961) with: {'batch_size': 60, 'epochs': 100}\n",
      "0.102032 (0.039122) with: {'batch_size': 80, 'epochs': 10}\n",
      "0.095017 (0.040913) with: {'batch_size': 80, 'epochs': 50}\n",
      "0.099556 (0.037432) with: {'batch_size': 80, 'epochs': 100}\n",
      "0.094708 (0.046992) with: {'batch_size': 100, 'epochs': 10}\n",
      "0.077272 (0.030808) with: {'batch_size': 100, 'epochs': 50}\n",
      "0.079129 (0.045573) with: {'batch_size': 100, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [1, 10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best optimizer, BEST : Nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer='adam'):\n",
    "    # create model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Conv1D(filters=12, kernel_size=5, activation='relu',input_shape=(n_features, 1)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(8, input_dim=199, activation='relu'))\n",
    "    model.add(Dense(14, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.123285 using {'optimizer': 'Nadam'}\n",
      "0.098937 (0.029671) with: {'optimizer': 'SGD'}\n",
      "0.040751 (0.044148) with: {'optimizer': 'RMSprop'}\n",
      "0.075003 (0.065321) with: {'optimizer': 'Adagrad'}\n",
      "0.087795 (0.067563) with: {'optimizer': 'Adadelta'}\n",
      "0.041164 (0.058214) with: {'optimizer': 'Adam'}\n",
      "0.069328 (0.028388) with: {'optimizer': 'Adamax'}\n",
      "0.123285 (0.090091) with: {'optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "#create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=1, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best learning rate, BEST : 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(learn_rate=0.01, momentum=0):\n",
    "    # create model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Conv1D(filters=12, kernel_size=5, activation='relu',input_shape=(n_features, 1)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(8, input_dim=199, activation='relu'))\n",
    "    model.add(Dense(14, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = tf.keras.optimizers.Nadam(lr=learn_rate)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.136078 using {'learn_rate': 0.01}\n",
      "0.093882 (0.043343) with: {'learn_rate': 0.001}\n",
      "0.136078 (0.085941) with: {'learn_rate': 0.01}\n",
      "0.000000 (0.000000) with: {'learn_rate': 0.1}\n",
      "0.083153 (0.117596) with: {'learn_rate': 0.2}\n",
      "0.026308 (0.037205) with: {'learn_rate': 0.3}\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=1, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "param_grid = dict(learn_rate=learn_rate)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best Network Weight Initialization, BEST: he_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(init_mode='uniform'):\n",
    "    # create model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Conv1D(filters=12, kernel_size=5, activation='relu',kernel_initializer=init_mode,input_shape=(n_features, 1)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=5, activation='relu',kernel_initializer=init_mode))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(8, input_dim=199, activation='relu'))\n",
    "    model.add(Dense(14, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = tf.keras.optimizers.Nadam(lr=0.01)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.104969 using {'init_mode': 'he_uniform'}\n",
      "0.045013 (0.063658) with: {'init_mode': 'uniform'}\n",
      "0.083792 (0.118499) with: {'init_mode': 'lecun_uniform'}\n",
      "0.083792 (0.118499) with: {'init_mode': 'normal'}\n",
      "0.083792 (0.118499) with: {'init_mode': 'zero'}\n",
      "0.083792 (0.118499) with: {'init_mode': 'glorot_normal'}\n",
      "0.083792 (0.118499) with: {'init_mode': 'glorot_uniform'}\n",
      "0.083792 (0.118499) with: {'init_mode': 'he_normal'}\n",
      "0.104969 (0.106724) with: {'init_mode': 'he_uniform'}\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=1, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "param_grid = dict(init_mode=init_mode)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best Neuron Activation Function, BEST: 'tanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(activation='relu'):\n",
    "    # create model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Conv1D(filters=12, kernel_size=5, activation=activation,kernel_initializer='he_uniform',input_shape=(n_features, 1)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=5, activation=activation,kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(8, input_dim=199, activation=activation))\n",
    "    model.add(Dense(14, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = tf.keras.optimizers.Nadam(lr=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.101393 using {'activation': 'tanh'}\n",
      "0.083792 (0.118499) with: {'activation': 'softmax'}\n",
      "0.083792 (0.118499) with: {'activation': 'softplus'}\n",
      "0.066832 (0.032525) with: {'activation': 'softsign'}\n",
      "0.083792 (0.118499) with: {'activation': 'relu'}\n",
      "0.101393 (0.108222) with: {'activation': 'tanh'}\n",
      "0.083792 (0.118499) with: {'activation': 'sigmoid'}\n",
      "0.042171 (0.059639) with: {'activation': 'hard_sigmoid'}\n",
      "0.085900 (0.046808) with: {'activation': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=1, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "param_grid = dict(activation=activation)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best Dropout Regularization + weight constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.constraints import maxnorm\n",
    "\n",
    "def create_model(dropout_rate=0.5):#, weight_constraint=0):\n",
    "    # create model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Conv1D(filters=12, kernel_size=5, activation='tanh',kernel_initializer='he_uniform',input_shape=(n_features, 1))) # kernel_constraint=maxnorm(weight_constraint),\n",
    "    model.add(Conv1D(filters=64, kernel_size=5, activation='tanh',kernel_initializer='he_uniform' ))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(8, input_dim=199, activation='tanh'))\n",
    "    model.add(Dense(14, activation='softmax'))\n",
    "    # Compile model\n",
    "    optimizer = tf.keras.optimizers.Nadam(lr=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.134213 using {'dropout_rate': 0.0}\n",
      "0.134213 (0.114341) with: {'dropout_rate': 0.0}\n",
      "0.042171 (0.059639) with: {'dropout_rate': 0.1}\n",
      "0.096535 (0.110171) with: {'dropout_rate': 0.2}\n",
      "0.078841 (0.062316) with: {'dropout_rate': 0.3}\n",
      "0.122937 (0.101871) with: {'dropout_rate': 0.4}\n",
      "0.057297 (0.022224) with: {'dropout_rate': 0.5}\n",
      "0.000000 (0.000000) with: {'dropout_rate': 0.6}\n",
      "0.122021 (0.102754) with: {'dropout_rate': 0.7}\n",
      "0.041896 (0.058667) with: {'dropout_rate': 0.8}\n",
      "0.125504 (0.091622) with: {'dropout_rate': 0.9}\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=1, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "#weight_constraint = [0,1, 2, 3, 4, 5]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "param_grid = dict(dropout_rate=dropout_rate)#, weight_constraint=weight_constraint)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best Number of Neurons in the Hidden Layer, BEST: 105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(neurons=8):\n",
    "    # create model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Conv1D(filters=12, kernel_size=5, activation='tanh',kernel_initializer='he_uniform',input_shape=(n_features, 1)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=5, activation='tanh',kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(neurons, input_dim=199, activation='tanh'))\n",
    "    model.add(Dense(14, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = tf.keras.optimizers.Nadam(lr=0.01)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.118995 using {'neurons': 105}\n",
      "0.102494 (0.056236) with: {'neurons': 95}\n",
      "0.077833 (0.013030) with: {'neurons': 100}\n",
      "0.118995 (0.074054) with: {'neurons': 105}\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=1, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "neurons = [95, 100, 105]\n",
    "param_grid = dict(neurons=neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
